== 分库分表

=== 分表的根本原因

分库分表应该拆分为分表、分库，一般来说先进行分表，分表的原动力在于MySQL的单表性能问题，据说Mysql单表数据量超过N千万、或者表Size大于N+G性能就不行了。这个说法背后的逻辑是数据量超过一定大小，B+Tree索引高度就会增加，而没增加一层高度，整个索引扫描就会多一次IO。其实更关键在于应用本身的使用，如果多数是索引命中率很高的点查或小范围查，其实这个上线还很高。但正是业务的不可控，所以大家往往采取比较保守的策略，这就是分表的原因。

=== 分库+分表的根本原因

分库主要由于MySQL容量上，Mysql的写入是很昂贵的操作，它本身有很多优化技术，即使如此，写入也存在放大很多倍的现象。同时Mysql M-S的架构虽然天然地支持读流量扩展，但由于Mysql从库默认采用单线程的SQL thread进行Binlog顺序重放，这种单线程的从库写入极大地限制整个集群的写入能力，（除非不在意数据延迟，而数据延迟与否直接影响了读数据的可用性）。Mysql基于组替吉奥的并行复制从某种程度上缓解了这个问题，但本质上写入上限还是非常容易达到（实际业务几千的TPS），目前一些云RDS通过计算存储分离、log is database的理念来很大程度上解决了写入扩大的问题，但在这之前，更为普遍的解决方案就是把一个集群拆分为N个集群，即分库分表（sharding）。为了规避热点问题，绝大多数采用的方法就是hash切分，也有极少的范围、或者基于Mapping的查询切分。

== 分库分表方案

=== DB Proxy

DBproxy 高度依赖网络组件，它需要诸如LVS/F5等VIP来实现流量的负载均衡，如果跨IDC，还依赖诸如DNS进行IDC分发。同时部分DBproxy对Prepare这类操作支持不友好，所以概括来说：

. 链路过长，每层都会增加响应时间
. 网络单点，并且往往是整个公司层面的单点
. 部分产品对Prepare应用不友好，需要绑定connection信息

==== Mycat

> 基于proxy，复写MySQL协议，将Mycat Server伪装成一个MySQL数据库

缺点：

1. 需要独立维护高可用性

== JDBC Proxy

JDBC Proxy最大的问题是违背了DB透明的原则，它需要对不同的语言编写Driver，概括来说：

* 语言限制
* 接入繁琐
* DB 不透明

=== Sharding-JDBC

> 基于JDBC接口的扩展，是以jar包的形式提供轻量级服务


== Sharding成本汇总

=== 应用限制

* DB弱化成存储
* 跨DB事务、全局约束难实现
* SQL聚合，join，子查询限制

=== 拆分Key不易选择

=== 算法选择

* Hash、Range、Mapping每种算法都有短板
* 算法转换成本极大

=== 强一致性无法保证

=== 业务多维度

* 数据冗余
* 数据一致性保证
* 数据同步
** 双写
** 异步同步
** Canal、Databus、MQ、DataX

=== 全局ID复杂实现

=== 高可用扩散问题

* MGR、PXC都有各自的问题
* MHA切换准确率、时间成本

=== 弹性、再扩容成本超大

=== 容量与资源成本

* 分少了需要再扩容，分多了浪费资源

=== 改造兼容性

* 业务能等到时间很短
* 需要高昂的技术储备

=== 运维成本

* DDL变更更重，OSC工具存在隐患
* 需要自动化运维

=== OLAP业务需要重复资源

* 人力
* 产品：ETL、Hive、HBase、Hadoop


分库分表为了解决一个问题，引入了很多成本，从长久看这种方案会逐步被新的解决方案替代。

* 第一个思路既然分库的原动力主要是单实例的写入容量限制，那么我们可以最大程度地提升整个写入容量，云计算的发展为这种思路提供了新的可能，以AWS Aurora为代表RDS，它以Log is database为理念，将复杂的随机读写简化为顺序写到Log，并通过将计算与存储分离，把复杂的数据持久化、一致性、数据合并都扔给高可用的共享存储系统来完成，进而打开写入的天花板，将昂贵的写入容量提升一个量级。

* 第二种思路承认分片的必要性，将这种分片策略集成到一套整体的分布式数据库体系中，同时通过Paxos/Raft复制协议和加上多实例节点来实现数据一致的高可用，其中代表产品有Google点Spanner&F1、Tidb、CockRoachDB等，是比较理想的Sharding+Proxy的替代方案。