:imagesdir: ../../diagram/drawio
== cap

zookeep保证CP，既任何时刻对zookeeper的访问请求能得到一致性的数据结果，同时系统对网络分割具备容错性，但是它不能保证每次服务的可用性。从实际情况来分析，在使用zookeeper获取服务列表是，如果zk正在选举或者zk集群中半数以上机器不可用，那么将无法获取数据。所以说zk不能保证可用性。

image::registry_zk.dio.svg[]


=== 分布式锁

首先zk的模式是CP模型，也就是说，当zk锁提供给我们进行访问的时候，在zk集群中能确保这把锁在zk的每一个节点都存在。（这个实际上是zk的leader通过二阶段提交写请求来保证的，这个也是zk的集群规模大了点一个瓶颈点）

zookeeper的分布式锁要比redis可靠很多，但他频繁的实现机制导致了它的性能不如redis，而且zk会随这集群的扩大而性能更加下降。

==== zk锁实现的原理

说zk的锁问题之前先看看zookeeper中几个特性，这几个特性构建了zk点一把分布式锁

特性:

* 有序节点

当在一个父目录下如/lock下创建有序节点，节点会按照严格的先后顺序创建出自节点lock000001,lock000002,lock000003以此类推，有序节点能严格保证各个自几点按照排序命名生成。

* 临时节点

客户端建立了一个临时节点，在客户端的会话结束或会话超时，zookeeper会自动删除该节点。

* 事件监听

在我们读取数据时，我们可以对节点设置监听，当节点的数据发生变化（1节点创建，2节点删除，3节点数据修改）时，zookeeper会通知客户端。

结合这几个特点，来看下zk是怎么组合分布式锁的。

image::registry_zk_lock.dio.svg[]

业务线程1抢到了/lock0001的文件，也就是在整个目录下最小序的节点，也就是线程-1获取到了锁，业务线程-1与lock0001建立了连接，并维持了心跳，维持的心跳也就是这把锁的租期。
当业务线程-1完成了业务，将释放掉与zk的链接，也就是释放了这把锁。

== 附录

https://juejin.cn/post/6844903936718012430[神一样的CAP理论被应用在何方]