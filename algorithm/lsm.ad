:imagesdir: ../static/img
== Log Structured Merge Trees(LSM)

> 日志结构的合并树

简单来说，LSM被设计来提供比传统的B+树或者ISAM更好的写操作吞吐量，通过消去随机的本地更新操作来达到这个目标(顺序读写快，随机读写慢)。

因为简单和高效，基于日志的策略在大数据之间越来越流行，同时他们也有一些缺点，从日志中读一些数据将会比写操作需要更多的时间，需要倒序扫描，直接找到所需要的内容。

这说明日志仅适用于一些简单的读场景：

. 数据是整体访问，像大部分数据库的WAL（write-ahead log）
. 知道明确的offset，比如在kafka中

所以需要更多的日志来为更复杂的读场景（比如按key或者range）提供高效的性能，这儿有4个方法可以完成这个，他们分别是：

. 二分查找：将文件数据有序保存，使用二分查找来完成特定key的查找。
. 哈希：用哈希将数据分割为不同的bucket
. B+树：使用B+树或者ISAM等方法，可以减少外部文件的读取
. 外部文件：将数据保存为日志，并创建一个hash或者查找树映射相关的文件

=== The Base LSM Algorithm

从概念上说，最基本的LSM是很简单的。将之前使用一个大的查找结构（造成随机读写，影响写性能），变换为将写操作顺序的保存到一些相似的有序文件（也就是sstable）中。所以每个文件包含短时间内的一些改动。因为文件是有序的，所以以后查找也会很快。文件是不可修改的，他们永远不会被更新，新的操作只会写到新的文件中。读操作检查所有的文件。通过周期性的合并这些文件来减少文件个数。

image::ds_lsm.dio.svg[]

LSM的原理：将对数据的修改增量保存在内存中，达到指定大小限制之后批量把数据flush到磁盘中，磁盘中树定期可以做merge操作，合并成一棵大树，以优化读性能。不过读取的时候稍微麻烦一些，读取时看这些数据在内存中，如果未能命中内存，则需要访问较多的磁盘文件。

写入不占用磁盘的io，读取就能获取更长时间的磁盘io使用权，从而也可以提升读取效率。

=== 性能优化

当查找数据库中某个不存在的key时，LSM-Tree可能很慢：在确定键不存在之前，必须先检查内存表，然后将段一直 回溯访问到最旧的段文件（可能必须从磁盘多次读取）。为了优化这种访问，存储引擎通常使用额外的布隆过滤器（布隆过滤器是内存高效的数据结构，用于近似计算集合的内容。如果数据库中不存在某个键，它能够很快告诉你结果，从而节省了很多对于不存在的键的不必要的磁盘读取）。

还有不同的压缩策略会影响甚至决定SSTables压缩和合并时的具体顺序时机。最常见的方式是大小分级和分层压缩。LevelDB和RockesDB使用分层压缩，HBase使用大小分级Cassdandra则同时支持这两种压缩。在大小分级中新的较小的SSTables会被不断合并到较旧较大的SSTables。在分层压缩，健的范围分裂成多个更小的SSTables，旧数据被移动到单独的“层级”，这样压缩可以逐步进行并节省磁盘空间。

=== LSM-tree的缺点



### 附录

. 磁盘的理论速度 200-300 MB/s
. 支持copy-on-write tree（通过顺序在文件末尾重复写对结构来实现写操作，之前的树结构相关部分，包括最顶层节点都会变成孤节点。尽管通过这种方法避免了本地更新，但是因为每个操作都要重写树结构，放大了写操作，降低了写性能）。
. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.44.2782&rep=rep1&type=pdf[LSM 论文]